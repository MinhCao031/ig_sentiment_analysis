{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMWJy8xBrw2sPZo1FTwq+n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Install libraries"],"metadata":{"id":"NKHTxNkAoCku"}},{"cell_type":"code","source":["from IPython.display import clear_output\n","!pip install alibi[tensorflow]\n","clear_output()"],"metadata":{"id":"3nbbUmb0p-ZV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers\n","clear_output()"],"metadata":{"id":"oum6qPkMqQtk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import libraries"],"metadata":{"id":"d4v-nAvgoGfH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LVTi58dDlAUd"},"outputs":[],"source":["# Import necessary modules\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import re\n","from alibi.explainers import IntegratedGradients\n","import matplotlib as plt\n","\n","# Render color to explain\n","from IPython.display import HTML"]},{"cell_type":"code","source":["%whos"],"metadata":{"id":"YAdneBHKimxv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702487035121,"user_tz":-420,"elapsed":20,"user":{"displayName":"Minh Cao","userId":"13864015769759919332"}},"outputId":"47d65db6-8ed2-461c-9e71-5074ea46f234"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Variable              Type        Data/Info\n","-------------------------------------------\n","HTML                  type        <class 'IPython.core.display.HTML'>\n","IntegratedGradients   ABCMeta     <class 'alibi.explainers.<...>nts.IntegratedGradients'>\n","clear_output          function    <function clear_output at 0x7d2ba968a8c0>\n","np                    module      <module 'numpy' from '/us<...>kages/numpy/__init__.py'>\n","pd                    module      <module 'pandas' from '/u<...>ages/pandas/__init__.py'>\n","plt                   module      <module 'matplotlib' from<...>/matplotlib/__init__.py'>\n","re                    module      <module 're' from '/usr/lib/python3.10/re.py'>\n","tf                    module      <module 'tensorflow' from<...>/tensorflow/__init__.py'>\n"]}]},{"cell_type":"markdown","source":["# Define some functions"],"metadata":{"id":"TQvIwDu9qfuj"}},{"cell_type":"code","source":["# Preprocess and clean the reviews\n","def preprocess_reviews (reviews) :\n","  # Preprocess the text\n","  REPLACE_NO_SPACE = re.compile(\"[.;:,!\\'?\\\"()\\[\\]]\")\n","  REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n","\n","  reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n","  reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n","  return reviews"],"metadata":{"id":"ovbI-ILy_JSB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize the sentences for the BERT Model\n","def process_sentences(sentence, tokenizer, max_len):\n","  # Tokenize the text sentences.\n","  z = tokenizer(sentence,\n","                add_special_tokens = False,\n","                padding = 'do_not_pad',\n","                max_length = max_len,\n","                truncation = True,\n","                return_token_type_ids = False,\n","                return_attention_mask = True,\n","                return_tensors = 'np')\n","  # z = tokenizer.encode(sentence[0],\n","  #               add_special_tokens = True, # add [CLS], [SEP]\n","  #               padding = 'do_not_pad',\n","  #               max_length = max_len, # max length of the text that can go to BERT\n","  #               truncation = True,\n","  #               return_token_type_ids = False,\n","  #               return_attention_mask = True,\n","  #               return_tensors = 'np')\n","  return z"],"metadata":{"id":"mS8KSf18_NKH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define Wrapper for making sentiment predictions with pretrained BERT Model\n","class AutoModelWrapper(tf.keras.Model):\n","  def __init__(self, model_bert, **kwargs):\n","    super().__init__()\n","    self.model_bert = model_bert\n","\n","  def call(self, inputs, attention_mask=None):\n","    out = self.model_bert(inputs, attention_mask=attention_mask)\n","    return tf.nn.softmax(out.logits)\n","\n","  def get_config(self):\n","    return {}\n","\n","  @classmethod\n","  def from_config(cls, config):\n","    return cls(**config)"],"metadata":{"id":"xRWrXmb6_P8H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute hex colors based on the attributions for a single instance.\n","# Uses a diverging colorscale by default and normalizes and scales\n","# the colormap so that colors are consistent with the attributions.\n","def colorize(attrs, cmap='PiYG'):\n","  cmap_bound = np.abs(attrs).max()\n","  norm = plt.colors.Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n","  cmap = plt.cm.get_cmap(cmap)\n","\n","  # now compute hex values of colors\n","  colors = list(map(lambda x: plt.colors.rgb2hex(cmap(norm(x))), attrs))\n","  return colors"],"metadata":{"id":"ANTwNPXH_R7v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Return HTML markup highlighting text with the desired color.\n","def hlstr(string, color='white'):\n","  return f\"<mark style=background-color:{color}>{string} </mark>\""],"metadata":{"id":"GMjjjvRP_SEY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def replace_cls_sep(lst, replacement):\n","    for i in range(len(lst) - 1, -1, -1):\n","        if lst[i] != 0:\n","            lst[i] = replacement\n","            break\n","    lst[0] = replacement\n","    return lst\n","\n","def display_output(text_in_arr, tokenizer, baseline_type, max_len):\n","  # process_test_sample_2 = process_sentences(text_in_arr, tokenizer, max_len)\n","  process_test_sample_2 = process_sentences(preprocess_reviews(text_in_arr), tokenizer, max_len)\n","  # print(process_test_sample_2)\n","  x_test_sample_2 = process_test_sample_2['input_ids']\n","  kwargs_2 = {k: tf.constant(v) for k, v in process_test_sample_2.items() if k == 'attention_mask'}\n","  probability = auto_model(x_test_sample_2, **kwargs_2).numpy()\n","  predictions_2 = probability.argmax(axis=1)\n","\n","  # if baseline_type == \"blur\":\n","  #   baseline = get_blur_baseline(x_test_sample_2)\n","  # elif baseline_type == \"none\":\n","  #   baseline = None\n","  # else:\n","  #   baseline = None\n","\n","  baseline = None\n","  replace_cls_sep(kwargs_2['attention_mask'].numpy()[0], 0)\n","\n","  explanation_2 = ig.explain(\n","    X               = x_test_sample_2,\n","    forward_kwargs  = kwargs_2,\n","    baselines       = baseline,\n","    target          = predictions_2\n","  )\n","  attrs_2 = np.array(explanation_2.attributions[0]).sum(axis=2)\n","\n","  # Visualize the results\n","  words_2 = tokenizer.decode(x_test_sample_2[0]).split()\n","  attrs_2 = np.array(explanation_2.attributions[0])\n","  attrs_2 = attrs_2.sum(axis=2)\n","  colors_2 = colorize(attrs_2[0])\n","  # print(attrs_2[0])\n","  print('-----\\nPositive = {}% -> Label {}'.format(probability[0][1]*100, predictions_2))\n","  return display(HTML(\"\".join(list(map(hlstr, words_2, colors_2)))))"],"metadata":{"id":"EQZc4VR9szmn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%whos"],"metadata":{"id":"aj6UaxIJNIVg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702487035124,"user_tz":-420,"elapsed":13,"user":{"displayName":"Minh Cao","userId":"13864015769759919332"}},"outputId":"fe042571-d742-410c-f5a1-0d19f278143e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Variable              Type        Data/Info\n","-------------------------------------------\n","AutoModelWrapper      type        <class '__main__.AutoModelWrapper'>\n","HTML                  type        <class 'IPython.core.display.HTML'>\n","IntegratedGradients   ABCMeta     <class 'alibi.explainers.<...>nts.IntegratedGradients'>\n","clear_output          function    <function clear_output at 0x7d2ba968a8c0>\n","colorize              function    <function colorize at 0x7d2a59e509d0>\n","display_output        function    <function display_output at 0x7d2a59e508b0>\n","hlstr                 function    <function hlstr at 0x7d2a59e50280>\n","np                    module      <module 'numpy' from '/us<...>kages/numpy/__init__.py'>\n","pd                    module      <module 'pandas' from '/u<...>ages/pandas/__init__.py'>\n","plt                   module      <module 'matplotlib' from<...>/matplotlib/__init__.py'>\n","preprocess_reviews    function    <function preprocess_reviews at 0x7d2b8c5d5cf0>\n","process_sentences     function    <function process_sentences at 0x7d2a59e51090>\n","re                    module      <module 're' from '/usr/lib/python3.10/re.py'>\n","replace_cls_sep       function    <function replace_cls_sep at 0x7d2a59e50430>\n","tf                    module      <module 'tensorflow' from<...>/tensorflow/__init__.py'>\n"]}]},{"cell_type":"markdown","source":["# Setup Model and IG"],"metadata":{"id":"p0zbXosUqnln"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"OpbwjOCpsFDO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702487656014,"user_tz":-420,"elapsed":620900,"user":{"displayName":"Minh Cao","userId":"13864015769759919332"}},"outputId":"9b80dc6a-0536-4fb7-b238-8548f2924e3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["from transformers import TFAutoModelForSequenceClassification, BertTokenizer\n","\n","model_save_path = '/content/gdrive/MyDrive/ColabNotebooks/IG2/Saved_BERT_model'\n","tokenizer_save_path = '/content/gdrive/MyDrive/ColabNotebooks/IG2/Saved_BERT_model'\n","\n","# Load the saved model weights\n","auto_model_bert = TFAutoModelForSequenceClassification.from_pretrained(model_save_path)\n","\n","# Load the saved tokenizer\n","tokenizer = BertTokenizer.from_pretrained(tokenizer_save_path)"],"metadata":{"id":"NgqfREPvqrLT","colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"status":"error","timestamp":1702579449529,"user_tz":-420,"elapsed":9517,"user":{"displayName":"Minh Cao","userId":"13864015769759919332"}},"outputId":"36accbc0-3b38-402f-bd69-a843bb14e127"},"execution_count":1,"outputs":[{"output_type":"error","ename":"HFValidationError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-2203869089f2>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the saved model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mauto_model_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the saved tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    489\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         ):\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/gdrive/MyDrive/ColabNotebooks/IG2/Saved_BERT_model'. Use `repo_type` argument if needed."]}]},{"cell_type":"code","source":["auto_model_bert.summary()"],"metadata":{"id":"j6_BAJYqq3Rt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = 512"],"metadata":{"id":"YEPKc0pVrMRs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1st parameter for IG function\n","# Making sentiment predictions with pretrained BERT Model\n","auto_model = AutoModelWrapper(auto_model_bert)\n","\n","# 2nd parameter for IG function\n","# Layer with respect to which the gradients are calculated\n","block = auto_model.layers[0].layers[0].embeddings\n","\n","# 3rd parameter for IG function\n","# Method for the integral approximation\n","method = \"gausslegendre\"\n","\n","# 4th parameter for IG function\n","# Number of step in the path integral approximation from the baseline\n","n_steps = 32\n","\n","# 5th parameter for IG function\n","# Batch size for the internal batching\n","internal_batch_size = 1\n","\n","# IG function\n","ig = IntegratedGradients(\n","  model               = auto_model,\n","  layer               = block,\n","  method              = method,\n","  n_steps             = n_steps,\n","  internal_batch_size = internal_batch_size\n",")"],"metadata":{"id":"jNJfzP0Vhojc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(block)"],"metadata":{"id":"MiTqKmwcCqO9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Apply model and explain"],"metadata":{"id":"cNFfo4bsryF8"}},{"cell_type":"code","source":["# Test sentence for the sentiment analysis\n","# z_test_sample = ['This is the best movie i have ever watch, there is nothing bad to say about that film']\n","z_test_sample = ['I found the tech support to be very responsive and helpful']\n","\n","# Process and tokenize the sentences\n","z_test_sample = process_sentences(z_test_sample, tokenizer, max_len)\n","print(z_test_sample)"],"metadata":{"id":"9yjb3_8FmuW0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1st parameter for explain function\n","# Instance for which integrated gradients attribution are computed\n","x_test_sample = z_test_sample['input_ids']\n","\n","# 2nd parameter for explain function\n","# Get tensors for model prediction\n","kwargs = {k: tf.constant(v) for k, v in z_test_sample.items() if k == 'attention_mask'}\n","\n","# 4th parameter for explain function\n","# Get the prediction outputs\n","predictions = auto_model(x_test_sample, **kwargs).numpy().argmax(axis=1)\n","\n","# Explain function\n","explanation = ig.explain(\n","  X               = x_test_sample,\n","  forward_kwargs  = kwargs,\n","  baselines       = None,\n","  target          = predictions\n",")"],"metadata":{"id":"gN8_1PS6oWGY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions"],"metadata":{"id":"lC_YSP7dNhuC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualize results"],"metadata":{"id":"MJ0r49DMrta5"}},{"cell_type":"code","source":["attrs = np.array(explanation.attributions[0])\n","attrs = attrs.sum(axis=2)\n","print('Attributions shape: ', attrs.shape)\n","words = tokenizer.decode(x_test_sample[0]).split()\n","colors = colorize(attrs[0])"],"metadata":{"id":"yZ4lrPG7pfx3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Predicted label = {}'. format(predictions))\n","HTML(\"\".join(list(map(hlstr, words, colors))))"],"metadata":{"id":"-gaFyxgJp4Tg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attrs"],"metadata":{"id":"8gL9HJYg1DbS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extra data"],"metadata":{"id":"AfySBYtnsYdR"}},{"cell_type":"code","source":["import time\n","from IPython.display import display\n","\n","extra_single_sentences = [ [\"Bad\"],\n","  [\"The delivery was late and the product was damaged.\"],\n","  [\"The customer service at the restaurant was exceptional.\"],\n","  [\"The airline staff was not helpful during our travel delay.\"],\n","  [\"I found the tech support to be very responsive and helpful.\"],\n","  [\"The gym staff is always friendly and supportive.\"],\n","  [\"The service at the car repair shop was not satisfactory.\"],\n","  [\"I had a great experience with the customer service team.\"],\n","  [\"The cleaning service did an excellent job with my apartment.\"],\n","  [\"The customer support was unable to resolve my issue.\"],\n","  [\"The hotel provided excellent room service during our stay.\"],\n","  [\"The medical staff was caring and made the visit less stressful.\"],\n","  [\"I had a poor experience with the airport security staff.\"],\n","  [\"The service at the spa was exceptional and relaxing.\"],\n","  [\"The mechanic was honest and fixed my car efficiently.\"],\n","  [\"The food delivery was quick and the food was still hot.\"],\n","  [\"The taxi service was unreliable and the driver was impolite.\"],\n","  [\"The concert staff ensured a safe and enjoyable experience for everyone.\"],\n","  [\"The property management service is always responsive to our needs.\"],\n","  [\"The amusement park staff was very friendly and helpful.\"],\n","  [\"The delivery service did not handle the package with care.\"],\n","  [\"The security service at the event was not up to the mark.\"],\n","  [\"The staff at the post office was very helpful in resolving my issue.\"],\n","  [\"The customer service at the internet company was very poor.\"],\n","  [\"The hospital staff provided excellent care during my stay.\"],\n","  [\"The customer service at the phone company was not able to resolve my issue.\"],\n","  [\"The technical support was unable to fix the issue with my computer.\"],\n","  [\"The management at the hotel was not responsive to our complaints.\"],\n","  [\"The service at the pet store was excellent and the staff was very helpful.\"],\n","  [\"The staff at the grocery store was not friendly.\"],\n","  [\"The customer service at the online store was quick to respond and very helpful.\"]\n","]\n","\n","extra_double_sentences = [ [\"Good.\"],\n","  [\"Although the food was delicious, the service was slow and unresponsive.\"],\n","  [\"The hotel was clean and comfortable, but the staff was rude and unhelpful.\"],\n","  [\"Despite the high prices, the quality of the products was excellent.\"],\n","  [\"The customer service representative was friendly and knowledgeable, but the wait time on the phone was too long.\"],\n","  [\"Although the location was convenient, the room was small and cramped.\"],\n","  [\"The website was easy to navigate, but the checkout process was confusing and frustrating.\"],\n","  [\"Despite the crowded atmosphere, the bartender was attentive and made great drinks.\"],\n","  [\"The prices were reasonable, but the portion sizes were disappointingly small.\"],\n","  [\"Although the store was busy, the sales associate was patient and helpful.\"],\n","  [\"The spa was relaxing and rejuvenating, but the prices were a bit steep.\"],\n","  [\"Despite the long wait time, the doctor was thorough and attentive during the appointment.\"],\n","  [\"Although the delivery was fast, the food was cold and not as described.\"],\n","  [\"The gym was well-equipped and clean, but the music was too loud and distracting.\"],\n","  [\"Despite the limited menu options, the food was flavorful and well-prepared.\"],\n","  [\"Although the staff was friendly, the hotel room was not as clean as expected.\"],\n","  [\"The customer service was prompt and efficient, but the product was not as advertised.\"],\n","  [\"Despite the high price tag, the quality of the service was worth it.\"],\n","  [\"Although the restaurant was busy, the hostess was able to find us a table quickly.\"],\n","  [\"The store had a great selection of products, but the checkout line was long and slow-moving.\"],\n","  [\"Despite the inclement weather, the tour guide was knowledgeable and engaging.\"],\n","  [\"Although the hotel was in a great location, the noise level was too high to get a good night's sleep.\"],\n","  [\"The coffee shop had a cozy atmosphere, but the coffee itself was not very good.\"],\n","  [\"Despite the friendly staff, the food took a long time to arrive at the table.\"],\n","  [\"Although the website had a lot of information, it was difficult to find what I was looking for.\"],\n","  [\"The customer service team was able to resolve my issue quickly, but the problem should not have occurred in the first place.\"],\n","  [\"Despite the low prices, the quality of the products was surprisingly good.\"],\n","  [\"Although the restaurant had a unique atmosphere, the food was not very flavorful.\"],\n","  [\"The airline had comfortable seats and good in-flight entertainment, but the food was mediocre.\"],\n","  [\"Despite the long wait time, the doctor was able to diagnose my condition accurately.\"],\n","  [\"Although the hotel room was spacious, the air conditioning was not working properly.\"]\n","]\n","\n","extra_long_sentence = ['I recently had the opportunity to try out a service that left a lasting impression on me. From the moment I entered, the atmosphere was welcoming and comfortable. The staff members were attentive, friendly, and professional. They demonstrated excellent knowledge and expertise in their field, ensuring that every step of the service was executed flawlessly. The attention to detail was remarkable, and I appreciated the careful consideration given to my specific needs and preferences. The service itself was outstanding, providing me with a sense of satisfaction and contentment. I left feeling rejuvenated and grateful for the exceptional experience. I highly recommend this service to anyone seeking a truly memorable experience']\n"],"metadata":{"id":"JOjAP5QUsaXb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extra test"],"metadata":{"id":"reaMSISRsTaU"}},{"cell_type":"code","source":["display_output(extra_single_sentences[3], tokenizer, \"none\", max_len)"],"metadata":{"id":"dBhMQPMLT_n5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_output(extra_long_sentence, tokenizer, \"none\", max_len)"],"metadata":{"id":"LVIBRZinO-yQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for z_test_sample_2 in extra_single_sentences:\n","  display_output(z_test_sample_2, tokenizer, \"none\", max_len)"],"metadata":{"id":"CnQbRaOXKy5x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for z_test_sample_2 in extra_double_sentences:\n","  display_output(z_test_sample_2, tokenizer, \"none\", max_len)"],"metadata":{"id":"hlhwlNoZsnIQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in extra_single_sentences:\n","    test_extra_sentence = i[0]\n","    predict_input = tokenizer.encode(test_extra_sentence,\n","                                     truncation=True,\n","                                     padding=True,\n","                                     return_tensors=\"np\")\n","    # print(predict_input)\n","    tf_output = auto_model_bert.predict(predict_input)[0]\n","    tf_prediction = tf.nn.softmax(tf_output, axis=1)\n","    labels = ['Negative','Positive'] #(0:negative, 1:positive)\n","    label = tf.argmax(tf_prediction, axis=1)\n","    label = label.numpy()\n","    percentage = round(100*tf_prediction.numpy()[0][1], 4)\n","    print(percentage, \"\\n\", i[0], sep=\"\")"],"metadata":{"id":"yaDaoQsyBu_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in extra_single_sentences:\n","    test_extra_sentence = i[0]\n","    predict_input = tokenizer.encode(test_extra_sentence,\n","                                     truncation=True,\n","                                     padding=True,\n","                                     return_tensors=\"tf\")\n","    print(predict_input)\n","    tf_output = auto_model_bert.predict(predict_input)[0]\n","    tf_prediction = tf.nn.softmax(tf_output, axis=1)\n","    labels = ['Negative','Positive'] #(0:negative, 1:positive)\n","    label = tf.argmax(tf_prediction, axis=1)\n","    label = label.numpy()\n","    percentage = round(100*tf_prediction.numpy()[0][1], 4)\n","    print(percentage, \"\\n\", i[0], sep=\"\")"],"metadata":{"id":"TwaB9d_QDsxu"},"execution_count":null,"outputs":[]}]}